{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0202d4-ee09-4798-80c8-52f51f542713",
   "metadata": {},
   "source": [
    "# Multi-Factor Alpha Competition\n",
    "By James Zhang, Omkar Pathak, Suryaa (Fall 2022)\n",
    "\n",
    "### Required imports and prerequisite packages for this project\n",
    "Here are some resources to learn more about each one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e1e5f-41fe-442c-aef7-c0d54e1443ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import random\n",
    "import kneed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from IPython.core import display as ICD\n",
    "\n",
    "from sif.siftools.backtesters import full_backtesters as fb\n",
    "from sif.siftools import operators as op\n",
    "from sif.siftools import metrics as metrics\n",
    "from sif.sifinfra import sif_utils as su\n",
    "from sif.siftools.abstractalpha import AbstractAlpha\n",
    "#from sif.sifinsights.alpha_search import apply_metric_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a0f05-936a-4cfd-ad9f-e8cb26f906d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Disable warnings to reduce output spam in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6580b-068c-4788-a1e8-51ae6fe3d51e",
   "metadata": {},
   "source": [
    "# Pairs Trading\n",
    "distance-based approach where the criterion was choosing pairs that minimized the sum of squared deviations between the two normalized prices\n",
    "- downside, small variance of the spread and therefore limits overall profitability\n",
    "\n",
    "correlation - reflects short-term linear dependence, but the problem is that two securities that are correlated don't necessarily share this equilibrium relationship that we desire. \n",
    "- we have no way of knowing that divergences will reverse back to equilibrium\n",
    "- high divergence risk and potential losses\n",
    "- hgih correlation is not related to high cointegration\n",
    "\n",
    "cointegration guarantees that there exists an equilibrium s.t. are more confident that divergences will even out over time\n",
    "\n",
    "## Finding pairs\n",
    "1. the most brute force solution to finding pairs is comparing every stock against every other stock to determine if there exists a cointegrated relationship\n",
    "- computationally expensive\n",
    "- this could lead to restrictive discovery of pairs purely within the same sector\n",
    "--> how can we have flexible search combinations while not limiting the combinations of pairs to obviously similar solutions (ie. same sector)\n",
    "\n",
    "## Machine Learning\n",
    "1. Dimensionality reduction - find a compact representation for each security to reduce computation costs\n",
    " \n",
    "PCA - transforms a large number of variables into a smaller number of uncorrelated variables called principal components. PCA will reduce 500 daily stock prices to 50 vars, and the stocks will be clustered on these components.\n",
    "\n",
    "\n",
    "2. unsupervised learning - apply an appropriate clustering algo\n",
    "\n",
    "OPTICS - Ordering Points To Identify the Clustering Structure or KMeans\n",
    "\n",
    "3. define a set of rules (ARODS) to select pairs for trading\n",
    "\n",
    "https://israeldi.github.io/coursework/EECS545/545_Final_Project.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07674020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import yfinance as yf\n",
    "\n",
    "def obtainArticleContent(code, year, month, day):\n",
    "    inputData = []\n",
    "    date = str(year) + '-' + str(month) + '-' + str(day)\n",
    "    query = getQuery(code)\n",
    "\n",
    "    if datetime(year, month, day).weekday() >= 5:\n",
    "      return\n",
    "    \n",
    "    url = makeURL(date, query, 100)\n",
    "    data = getData(url)\n",
    "    if data == None:\n",
    "        return\n",
    "    for i in data:\n",
    "        tmpDict = dict(i)\n",
    "        try:\n",
    "            page = requests.get(tmpDict['url'])\n",
    "        except:\n",
    "            print('page failed')\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.content, \"lxml\") \n",
    "            all_tags = soup.find_all('p')\n",
    "            for i in all_tags:\n",
    "                try:\n",
    "                    inputData.append(i.get_text())\n",
    "                except Exception as inst:\n",
    "                    print('character error')\n",
    "    return inputData \n",
    "\n",
    "def getData(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        data = data['articles']\n",
    "        return data\n",
    "    except:\n",
    "        return\n",
    "\n",
    "def makeURL(query, date, pageSize):\n",
    "    return ('https://newsapi.org/v2/everything?'\n",
    "            'q=' + query + '&'\n",
    "            'from=' + date + '&'\n",
    "            'sortBy=popularity&'\n",
    "            'pageSize=' + str(pageSize) + '&'\n",
    "            'apiKey=13bd628fa8b548738d3b113d9442574e&'\n",
    "            'language=en')\n",
    "\n",
    "\n",
    "def getQuery(code):\n",
    "    return get_symbol(code).split(',')[0]\n",
    "\n",
    "def get_symbol(symbol):\n",
    "    tick = yf.Ticker(symbol)\n",
    "    print(tick.info['longName'])\n",
    "    return tick.info['longName']\n",
    "\n",
    "import mygrad as mg\n",
    "import numpy as np\n",
    "from mynn.layers.dense import dense\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mynn.optimizers.adam import Adam\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from noggin import create_plot\n",
    "\n",
    "mg.turn_memory_guarding_off()\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "\n",
    "        self.fc_h2y = dense(dim_recurrent, dim_output, weight_initializer=glorot_normal)\n",
    "        self.Uz = mg.Tensor(\n",
    "            np.random.randn(dim_input * dim_recurrent).reshape(dim_input, dim_recurrent)\n",
    "        )\n",
    "        self.Wz = mg.Tensor(\n",
    "            np.random.randn(dim_recurrent * dim_recurrent).reshape(\n",
    "                dim_recurrent, dim_recurrent\n",
    "            )\n",
    "        )\n",
    "        self.bz = mg.Tensor(np.random.randn(dim_recurrent))\n",
    "        self.Ur = mg.Tensor(\n",
    "            np.random.randn(dim_input * dim_recurrent).reshape(dim_input, dim_recurrent)\n",
    "        )\n",
    "        self.Wr = mg.Tensor(\n",
    "            np.random.randn(dim_recurrent * dim_recurrent).reshape(\n",
    "                dim_recurrent, dim_recurrent\n",
    "            )\n",
    "        )\n",
    "        self.br = mg.Tensor(np.random.randn(dim_recurrent))\n",
    "        self.Uh = mg.Tensor(\n",
    "            np.random.randn(dim_input * dim_recurrent).reshape(dim_input, dim_recurrent)\n",
    "        )\n",
    "        self.Wh = mg.Tensor(\n",
    "            np.random.randn(dim_recurrent * dim_recurrent).reshape(\n",
    "                dim_recurrent, dim_recurrent\n",
    "            )\n",
    "        )\n",
    "        self.bh = mg.Tensor(np.random.randn(dim_recurrent))\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        h = mg.nnet.layers.gru(\n",
    "            x,\n",
    "            self.Uz,\n",
    "            self.Wz,\n",
    "            self.bz,\n",
    "            self.Ur,\n",
    "            self.Wr,\n",
    "            self.br,\n",
    "            self.Uh,\n",
    "            self.Wh,\n",
    "            self.bh,\n",
    "        )\n",
    "        return self.fc_h2y(h[-1])\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self.fc_h2y.parameters + (\n",
    "        self.Uz, self.Wz, self.bz, self.Ur, self.Wr, self.br, self.Uh, self.Wh, self.bh)\n",
    "\n",
    "def to_glove(sentence):\n",
    "    out = []\n",
    "    for word in sentence.split():\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            out.append(glove[word])\n",
    "        except:\n",
    "            continue\n",
    "    if len(out) > MAXLEN:\n",
    "        out = out[:MAXLEN]\n",
    "    elif len(out) < MAXLEN:\n",
    "        for _ in range(len(out), MAXLEN):\n",
    "            out.append(np.zeros(50))\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    sentence = to_glove(sentence)\n",
    "    w = np.ascontiguousarray(np.swapaxes(np.array(sentence).reshape(1, 100, 50), 0, 1))\n",
    "    pred = Keys[np.argmax(model(w))]\n",
    "    print(pred)\n",
    "\n",
    "\n",
    "def predict(multiple_sentences):\n",
    "    good = 0\n",
    "    bad = 0\n",
    "    pred = 0\n",
    "    for sentence in multiple_sentences:\n",
    "        sentence = to_glove(sentence)\n",
    "        w = np.ascontiguousarray(np.swapaxes(np.array(sentence).reshape(1, 100, 50), 0, 1))\n",
    "        pred = np.argmax(model(w))\n",
    "        if pred==1:\n",
    "            good +=1\n",
    "        else:\n",
    "            bad += 1\n",
    "    \n",
    "    good = good / 800\n",
    "    if good > 8:\n",
    "      return false\n",
    "    else:\n",
    "      return true\n",
    "\n",
    "\n",
    "def initPrediction(code, month, day, year):\n",
    "    inputData = obtainArticleContent(code, year, month, day)\n",
    "    print('got data')\n",
    "    params = np.load(\"model.npy\", allow_pickle=True)\n",
    "    model = RNN(50, 16, 2)\n",
    "    MAXLEN = 100\n",
    "    model.fc_h2y.weight, model.fc_h2y.bias, model.Uz, model.Wz, model.bz, model.Ur, model.Wr, model.br, model.Uh, model.Wh, model.bh = (\n",
    "        params[0],\n",
    "        params[1],\n",
    "        params[2],\n",
    "        params[3],\n",
    "        params[4],\n",
    "        params[5],\n",
    "        params[6],\n",
    "        params[7],\n",
    "        params[8],\n",
    "        params[9],\n",
    "        params[10]\n",
    "    )\n",
    "    glove = KeyedVectors.load_word2vec_format(\"glove.6B.50d.txt.w2v\", binary=False)\n",
    "    return predict(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1343f7-2f34-434c-9790-558015cad8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsTrading_PValue_ML(AbstractAlpha):\n",
    "    def __init__(self, reset, npairs):\n",
    "        self.name = 'Pairs Trading - P Value'\n",
    "        self.lookback = 252\n",
    "        self.factor_list = ['close']\n",
    "        self.universe_size = 500\n",
    "        \n",
    "        self.pairs = None\n",
    "        self.keyPairs = []\n",
    "        self.reset = reset\n",
    "        self.npairs = npairs\n",
    "        self.holdings = np.zeros(self.universe_size)\n",
    "        self.day_counter = 0\n",
    "        self.count = 0\n",
    "        csvfile = open('constituents_csv.csv','rb')\n",
    "        csvFileArray = []\n",
    "        for row in csv.reader(csvfile, delimiter = '.'):\n",
    "            csvFileArray.append(row)\n",
    "    \n",
    "    def zscore(self, series):\n",
    "        return (series - series.mean()) / np.std(series)\n",
    "    \n",
    "    # this function was created by Delanie MacKensie and posted on Quanttopia\n",
    "    def form_pairs(self, df):\n",
    "        df = pd.DataFrame(df).dropna()\n",
    "        n = df.shape[1]\n",
    "        \n",
    "        # creating an adjacency matrix sort of for cointegration scores and pvalues\n",
    "        score_matrix = np.zeros((n, n))\n",
    "        pvalue_matrix = np.ones((n, n))\n",
    "        keys = df.keys()\n",
    "        pairs = []\n",
    "        pairsDict = {} #maps from pvalue to pair\n",
    "        \n",
    "        # this nested for loop part can be optimized with ML\n",
    "        # for the sake of time, I don't loop through the entire universe size\n",
    "        # but this is definitely one of the areas of improvement\n",
    "        for i in range(150):\n",
    "            for j in range(i + 1, 200):\n",
    "                S1 = df[keys[i]]        \n",
    "                S2 = df[keys[j]]\n",
    "                result = coint(S1, S2)\n",
    "                score = result[0]\n",
    "                pvalue = result[1]\n",
    "                score_matrix[i, j] = score\n",
    "                pvalue_matrix[i, j] = pvalue\n",
    "                \n",
    "                # used a hashmap, sorted keys (pvalues) in ascending order in order\n",
    "                # to get the smallest pvalues\n",
    "                if pvalue < 0.05:\n",
    "                    pairsDict[pvalue] = ([i,j])\n",
    "                    #self.keyPairs.append((keys[i], keys[j]))\n",
    "        \n",
    "        OrderedPairs = OrderedDict(sorted(pairsDict.items()))\n",
    "        if len(OrderedPairs) < self.npairs:\n",
    "            pairs = OrderedPairs.values()\n",
    "        else:\n",
    "            pairs = list((OrderedPairs.values()))[:self.npairs]\n",
    "        \n",
    "        #print(f\"PValue Cointegrated Pairs - Round {self.count} - Pairs are {pairs}\")\n",
    "        #self.count += 1\n",
    "        return pairs\n",
    "    \n",
    "    def generate_day(self, day, data):\n",
    "        \n",
    "        # creating new pairs\n",
    "        if self.day_counter == 0:\n",
    "            self.day_counter = self.reset\n",
    "            self.pairs = self.form_pairs(data['close'])\n",
    "            return op.weight(self.holdings)\n",
    "      \n",
    "        data = pd.DataFrame(data['close'])\n",
    "        for p in self.pairs:\n",
    "            diff = data[p[0]] - data[p[1]]\n",
    "            \n",
    "            #zscore tells us how far from away from the mean a data point is\n",
    "            z_score = self.zscore(diff).tail(1).values[0]\n",
    "            \n",
    "            bool = initPrediction(csvFileArray[p[0]], day.month(), day.day(), day.year())\n",
    "            \n",
    "            # enter the trade, short the FIRST, long SECOND\n",
    "            if z_score > 1.0 and bool:\n",
    "                self.holdings[p[0]] = -1\n",
    "                self.holdings[p[1]] = 1   \n",
    "            # exit the trade\n",
    "            elif abs(z_score) < 0.5:\n",
    "                self.holdings[p[0]] = 0\n",
    "                self.holdings[p[1]] = 0\n",
    "            # enter the trade; long the FIRST, short SECOND\n",
    "            elif z_score < -1.0 and not bool:\n",
    "                self.holdings[p[0]] = 1\n",
    "                self.holdings[p[1]] = -1\n",
    "            \n",
    "        # at the end of the trading day, decrement day_counter\n",
    "        self.day_counter -= 1\n",
    "        return op.weight(self.holdings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345faa7",
   "metadata": {},
   "source": [
    "# Use below for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsTrading_PValue(AbstractAlpha):\n",
    "    def __init__(self, reset, npairs):\n",
    "        self.name = 'Pairs Trading - P Value'\n",
    "        self.lookback = 252\n",
    "        self.factor_list = ['close']\n",
    "        self.universe_size = 500\n",
    "        \n",
    "        self.pairs = None\n",
    "        self.keyPairs = []\n",
    "        self.reset = reset\n",
    "        self.npairs = npairs\n",
    "        self.holdings = np.zeros(self.universe_size)\n",
    "        self.day_counter = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def zscore(self, series):\n",
    "        return (series - series.mean()) / np.std(series)\n",
    "    \n",
    "    # this function was created by Delanie MacKensie and posted on Quanttopia\n",
    "    def form_pairs(self, df):\n",
    "        df = pd.DataFrame(df).dropna()\n",
    "        n = df.shape[1]\n",
    "        \n",
    "        # creating an adjacency matrix sort of for cointegration scores and pvalues\n",
    "        score_matrix = np.zeros((n, n))\n",
    "        pvalue_matrix = np.ones((n, n))\n",
    "        keys = df.keys()\n",
    "        pairs = []\n",
    "        pairsDict = {} #maps from pvalue to pair\n",
    "        \n",
    "        # this nested for loop part can be optimized with ML\n",
    "        # for the sake of time, I don't loop through the entire universe size\n",
    "        # but this is definitely one of the areas of improvement\n",
    "        for i in range(150):\n",
    "            for j in range(i + 1, 200):\n",
    "                S1 = df[keys[i]]        \n",
    "                S2 = df[keys[j]]\n",
    "                result = coint(S1, S2)\n",
    "                score = result[0]\n",
    "                pvalue = result[1]\n",
    "                score_matrix[i, j] = score\n",
    "                pvalue_matrix[i, j] = pvalue\n",
    "                \n",
    "                # used a hashmap, sorted keys (pvalues) in ascending order in order\n",
    "                # to get the smallest pvalues\n",
    "                if pvalue < 0.05:\n",
    "                    pairsDict[pvalue] = ([i,j])\n",
    "                    #self.keyPairs.append((keys[i], keys[j]))\n",
    "        \n",
    "        OrderedPairs = OrderedDict(sorted(pairsDict.items()))\n",
    "        if len(OrderedPairs) < self.npairs:\n",
    "            pairs = OrderedPairs.values()\n",
    "        else:\n",
    "            pairs = list((OrderedPairs.values()))[:self.npairs]\n",
    "        \n",
    "        #print(f\"PValue Cointegrated Pairs - Round {self.count} - Pairs are {pairs}\")\n",
    "        #self.count += 1\n",
    "        return pairs\n",
    "    \n",
    "    def generate_day(self, day, data):\n",
    "        \n",
    "        # creating new pairs\n",
    "        if self.day_counter == 0:\n",
    "            self.day_counter = self.reset\n",
    "            self.pairs = self.form_pairs(data['close'])\n",
    "            return op.weight(self.holdings)\n",
    "      \n",
    "        data = pd.DataFrame(data['close'])\n",
    "        for p in self.pairs:\n",
    "            diff = data[p[0]] - data[p[1]]\n",
    "            \n",
    "            #zscore tells us how far from away from the mean a data point is\n",
    "            z_score = self.zscore(diff).tail(1).values[0]\n",
    "            \n",
    "            \n",
    "            # enter the trade, short the FIRST, long SECOND\n",
    "            if z_score > 1.0:\n",
    "                self.holdings[p[0]] = -1\n",
    "                self.holdings[p[1]] = 1   \n",
    "            # exit the trade\n",
    "            elif abs(z_score) < 0.5:\n",
    "                self.holdings[p[0]] = 0\n",
    "                self.holdings[p[1]] = 0\n",
    "            # enter the trade; long the FIRST, short SECOND\n",
    "            elif z_score < -1.0:\n",
    "                self.holdings[p[0]] = 1\n",
    "                self.holdings[p[1]] = -1\n",
    "            \n",
    "        # at the end of the trading day, decrement day_counter\n",
    "        self.day_counter -= 1\n",
    "        return op.weight(self.holdings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647c289-801d-4566-836d-8e88ebf0945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [PairsTrading_PValue(120, 5)]\n",
    "# , PairsTrading_CointScore(120,5) 'Pairs Trading - Cointegration Score'\n",
    "returns, holdings = backtester.backtest(alphas, processes=None)\n",
    "\n",
    "metrics.summary_plot(returns, source=['Pairs Trading - P Value'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
